{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic TensorFlow\n",
    "\n",
    "Use tf constants and tf functions to do basic math operations. Also use feed_dict to allow user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "c = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print z from a session\n",
    "z = tf.subtract(tf.divide(x,y),c)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z,feed_dict={x:10,y:2,c:1})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function in TensorFlow\n",
    "\n",
    "Implement a simple calculation of inputs, weights, and a bias, \n",
    "\n",
    " $$ y = xW + b$$\n",
    " \n",
    " We need to use a tensor that is modifiable, so instead of using tf.constant() or tf.placeholder() we're going to use tf.Variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # returns operation that will initialize variables from graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to initialize variables from a normal distribution to prevent any one variable from overwhelming the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "\n",
    "#initialize Normally distributed weights\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "#initialize bias variable as a tensor of bunch of zeros\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65900117  0.24243298  0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax,feed_dict={logits:logit_data})\n",
    "    \n",
    "    return output\n",
    "\n",
    "output = run()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Coding\n",
    "Transform labels into binary vectors using scikit learn LabelBinarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy in Tensorflow\n",
    "Cross-entropy, $D$, is an error measurement for categorical outputs. This is computed by\n",
    "\n",
    "<img src=\"./www/cross-entropy-diagram.png\" alt=\"cross-entropy\" style=\"width: 400px;\"/>\n",
    "\n",
    "where $y$ is the one-hot coded ground truth vector, and $\\hat{y}$ is our probability output from the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "DD = -tf.reduce_sum(tf.multiply(one_hot_data,tf.log(softmax_data)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    output = sess.run(DD,feed_dict={softmax:softmax_data,one_hot:one_hot_data})\n",
    "    \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Stability\n",
    "When computing the gradient, we must ensure that the values we output are not too large or too small. A computer's precision is finite so things can quickly explode and overflow, or become too small and effectively be treated as zero.\n",
    "\n",
    "Example below: take a = 1 billion, add 10^-6 one million times, then subtract 1 billion. We obviously expect a result of 1, but.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is actually less than one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A rule of thumb:\n",
    "Always have our variables have a mean of zero and equal variance whenever possible, that is:\n",
    "\n",
    "\\begin{align} \\mu(X_i) &= 0\\\\\n",
    "\\sigma(X_i) &= \\sigma(X_j)\\end{align}\n",
    "\n",
    "<img src=\"./www/conditioning-vars.png\" alt=\"conditioning-variables\" style=\"width: 600px;\"/>\n",
    "\n",
    "Without properly conditioning our variables (left), our gradient descent algorithm may not find the minimum  in an efficient fashion.\n",
    "\n",
    "In the case of images, where (R,G,B) pixel values range from 0-250, we should subtract 128 and scale each pixel by $\\frac{1}{128}$.\n",
    "\n",
    "$$(R,G,B) \\rightarrow \\left(\\frac{R-128}{128}, \\frac{G-128}{128}, \\frac{B-128}{128}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "<img src=\"./www/weight-initialization.png\" alt=\"weight-initialization\" style=\"width: 500px;\"/>\n",
    "\n",
    "Draw the weights randomly from a Gaussian distribution with $\\mu = 0$ and standard deviation $\\sigma$. $\\sigma$ determines the order of magnitude of the initial points of our optimization. Because of the softmax() on top of our function, this order of magnitude determines the \"peakiness\" of our initial probability distribution. Large $\\sigma$ will result in large peaks, and a very opinionated distribution. A small $\\sigma$ means that our distribution will be very uncertain. \n",
    "\n",
    "It is better to start with an uncertain (small $\\sigma$) distribution, and let our distribution become more certain with more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "One major issue with scaling gradient descent: computing a loss function's gradient takes ~3x as long as computing the loss function itself. And the loss function is huge -- it depends on every single element in the training set. We must also do this many times as we perform many iterations over the dataset.\n",
    "\n",
    "In practice, we use a method called **stochastic gradient descent**. Instead of computing the loss, we compute an _estimate_ of it. We subsample the dataset with replacement and compute the average loss over that. \n",
    "\n",
    "<img src=\"./www/sgd.png\" alt=\"stochastic-gradient-descent\" style=\"width:500px\">\n",
    "\n",
    "Due to this being an estimate of the true error, we will be taking smaller steps (sometimes even in the wrong direction) towards the true optimum overall. Ultimately though, we are able to do these computations much faster than if we were to do the full gradient descent calculation, so in the end we win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helping S.G.D.\n",
    "**Inputs:**\n",
    "    - Mean=0\n",
    "    - equal variance (small)\n",
    "  \n",
    "**Weight initialization:**\n",
    "    - Random!\n",
    "    - mean =0\n",
    "    - equal variance (small)\n",
    "    \n",
    "**Momentum:**\n",
    "<img src=\"./www/momentum.png\" alt=\"momentum\" style=\"width:500px\">\n",
    "    - Instead of using the direction of the current batch gradient, keep a running average of the error\n",
    "    -this momentum technique works very well and often leads to better convergence\n",
    "\n",
    "**Learning rate decay:**\n",
    "<img src=\"./www/learningrate1.png\" alt=\"learning-rate\" style=\"width:500px\">\n",
    "    -Recall: s.g.d. yields smaller, noisier steps towards the optimum\n",
    "    -how small should the step be? this is an area of research\n",
    "    - people have shown it is best to make steps smaller and smaller as you train\n",
    "    - some apply exponential decay to learning rate\n",
    "    - some make it smaller when loss reaches plateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Tuning\n",
    "<img src=\"./www/learningratetuning.png\" alt=\"learning-rate-tuning\" style=\"width:500px\">\n",
    "    - unintuitively, lowering learning rate over time may lead to better performance than having a higher learning rate\n",
    "    - don't be deceived by how fast the loss converges\n",
    "    \n",
    "\n",
    "**Hyper-parameters:**\n",
    "    - initial learning rate\n",
    "    - learning rate decay\n",
    "    - momentum\n",
    "    - batch size\n",
    "    - weight initialization\n",
    "In practice it's not _that_ bad. One thing to keep in mind: try lowering learning rate _first_. \n",
    "\n",
    "**Adagrad:**\n",
    "    - modification of s.g.d. \n",
    "    - implicitly does momentum and learning rate decay\n",
    "    - makes learning less sensitive to hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching\n",
    "\n",
    "Technique to train on subsets of dataset instead of all data at once. Computationally this is inefficient since we can't calculate loss simultaneously over all samples, but we have the benefit of being able to run the model (some data sets are just way too large).\n",
    "\n",
    "Combined with SGD: shuffle the data at start of each epoch, then create mini batches. For each mini-batch, train network weights with gradient descent. Thus, we perform SGD with each batch since batches are random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    num_features = len(features)\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    num_batches = math.ceil(num_features/batch_size)\n",
    "    for bb in range(0,num_features,batch_size):\n",
    "        batches.append([features[bb:bb+batch_size],labels[bb:bb+batch_size]])\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you would call the code in practice: \n",
    "\n",
    "for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs\n",
    "One epoch is a single forward and backward pass of the whole dataset. \n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    #then loop over batches\n",
    "    for batch_features, batch_labels in train_batches:\n",
    "    train_feed_dict = ...\n",
    "    \n",
    "With each epoch, the cost becomes lower, and accuracy gets higher. We should adjust learning rate to see how it affects the accuracy over many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
