{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "For more reading refer to: \n",
    "\n",
    "- Andrej Karpathy's Stanford course\n",
    "    - http://cs231n.github.io/convolutional-networks/\n",
    "- Michael Neilsen's free book\n",
    "    - http://neuralnetworksanddeeplearning.com/\n",
    "- Goodfellow, Bengio, Courville's book\n",
    "    - http://deeplearningbook.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Considerations\n",
    "\n",
    "## Colour\n",
    "- If you are training a network to classify objects or letters\n",
    "- Colour may not be useful to the classifier and it will be faster to switch from RGB to grayscale\n",
    "\n",
    "## Statistical invariance\n",
    "- Similarly, the network should be able to classify an object regardless of where it is in an image\n",
    "    - translation invariance\n",
    "- Similarly in long text,a word's meaning (e.g. 'kitten') does not change meaning depending on where it is in the text\n",
    "- Neural networks accomplish this via **Weight Sharing**\n",
    "    - When two inputs can contain the same information \n",
    "    - Train the weights jointly for those inputs\n",
    "    - For images this will be the fundamental principle behind **Convolutional networks**\n",
    "    - For text, it will be the basis of **Embeddings** and **Recurrent neural networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "- i.e. convnets\n",
    "- Networks that share their parameters across space\n",
    "- Images usually have a width, height, and depth\n",
    "    - Depth = 3 channels for colour\n",
    "- Take a small patch (blue) of the image (white), and run a small neural network on it with `k` outputs (red)\n",
    "- This neural network will slide across the entire image without changing the weight\n",
    "- The new output image will have a new output image\n",
    "    - Different width, height, and **most importantly** a different depth\n",
    "    - Instead of RGB, we now have `K` channels\n",
    "- If the patch was the size of the image, it would be the same as a regular neural network\n",
    "- This smaller patch forces the layer to share weights\n",
    "- Basically has stacks of convolutions instead of matrix multiplying layers\n",
    "\n",
    "<img src=\"./images/week6/convolution.png\" alt=\"convolution\" style=\"width:250px\">    \n",
    "\n",
    "- Basic idea is to form a pyramid\n",
    "- Shallow RGB bottom \n",
    "- Apply convolutions that progressively squeeze the spatial dimensions \n",
    "- While increasing depth, which corresponds roughly to the semantic complexity of the representation\n",
    "\n",
    "<img src=\"./images/week6/convnet.png\" alt=\"pyramid\" style=\"width:400px\">  \n",
    "\n",
    "- At the top, we can put a **classifier**\n",
    "\n",
    "### Terminology\n",
    "- `Patches  = Kernels`\n",
    "- Each pancake layer of depth = 'feature map'\n",
    "    - RGB = 3 feature maps\n",
    "    - Map first layer from 3 to K feature maps \n",
    "- Stride = # pixels we move the kernel between each step\n",
    "    - tunable hyperparameter\n",
    "    - Stride of 1 leads to output roughly the same width and height as input\n",
    "    - Stride of 2 means it is about half the size\n",
    "    - **Valid padding**\n",
    "        - only striding within the boundaries of the image\n",
    "    - **Same padding**\n",
    "        - Shortcut that pads outsides with zeros \n",
    "        - Results in same output size as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter depth\n",
    "\n",
    "- Amount of filters in a convolutional layer\n",
    "- Dictates how many neurons each patch connects to\n",
    "- If we have `k`, we connect each patch of pixels to `k` neurons in the next layer\n",
    "- `k` is a tunable hyperparameter\n",
    "- Most CNNs tend to pick the same starting values\n",
    "\n",
    "<img src=\"./images/week6/filter-depth.png\" alt=\"dilterdepth\" style=\"width:200px\">  \n",
    "\n",
    "### Why connect a single patch to multiple neurons in next layer?\n",
    "\n",
    "- Can capture more interesting characteristics\n",
    "- Multiple neurons for a given  patch ensures the CNN will learn to capture whatever characteristics are important\n",
    "- The CNN is not explicitly programmed to look for characteristics\n",
    "- It **learns on its own**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strides, depth, and padding\n",
    "<img src=\"./images/week6/strides.png\" alt=\"dilterdepth\" style=\"width:400px\">  \n",
    "\n",
    "- If we start with an input image that is 28x28x3\n",
    "- Output depth is fixed to `k=8`\n",
    "- Same padding means the output width and height will be the same\n",
    "    - achieved by zero-padding\n",
    "- Valid padding means in order for out 3x3 patch to fit over the image, we must remove 1 row and 1 column from each side\n",
    "- Stride of 2 reduces the output size by one half\n",
    "\n",
    "## Implementation\n",
    "<img src=\"./images/week6/cnn-implementation.png\" alt=\"dilterdepth\" style=\"width:400px\">  \n",
    "\n",
    "- With each progressing layer, we take **larger strides** and increase `k`\n",
    "- Effectively reduces the width and height of the representation\n",
    "- And increases the depth\n",
    "- In the final step we can add a few fully connected layers\n",
    "- And add a classifier on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality\n",
    "- Given input volume has volume `W`\n",
    "- Filter has volume (height * width * depth) of `F`\n",
    "- Stride size `S`\n",
    "- Padding `P`\n",
    "- `new_height = (input_height - filter_height + 2 * P)/S + 1`\n",
    "- `new_width = (input_width - filter_width + 2 * P)/S + 1`\n",
    "- `new_depth = number of filters`\n",
    "- Volume of next layer = **`(W-F+2P)/S+1`**\n",
    "- Knowing dimensionality of each layer helps us unerstand how large our model is and how filter size and stride will affect the overall size of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_dim(input_dim, filt_dim, pad_size, stride_size):\n",
    "        new_out= (input_dim - filt_dim+2*pad_size)/stride_size + 1\n",
    "        \n",
    "        return new_out\n",
    "\n",
    "new_dim(32,8,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input = tf.placeholder(tf.float32,(None,32,32,3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8,8,3,20)))\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1,2,2,1] #batch, height, width, depth\n",
    "padding = 'VALID'\n",
    "conv = tf.nn.conv2d(input,filter_weights,strides,padding) + filter_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above conv will be [1,13,13,20]\n",
    "    - 4D\n",
    "- _Not_ [1,14,14,20] because padding algo in TF is not ehs ame as we've seen\n",
    "- Switching `padding` from `'VALID'` to `'SAME'`\n",
    "    - Output shape [1,16,16,20]\n",
    "\n",
    "More info on how TF does padding\n",
    "https://www.tensorflow.org/api_guides/python/nn#Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Sharing\n",
    "- Less parameters to optimize,\n",
    "- Which means faster convergence to some minima, \n",
    "- At the expense of making your model less flexible. It is interesting to note that, this \"less flexibility\" can work as a regularizer many times and avoiding overfitting as the weights are shared with some other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs\n",
    "\n",
    "http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf\n",
    "https://www.youtube.com/watch?v=ghEmQSxT6tw\n",
    "\n",
    "## Layer 1\n",
    "<img src=\"./images/week6/layer1.png\" alt=\"layer1\" style=\"width:200px\">  \n",
    "\n",
    "- Top left recognizes -45 degree lines\n",
    "- Top middle recognizes 45 degree lines\n",
    "- Also contains, green blobs, gabors\n",
    "\n",
    "## Layer 2\n",
    "<img src=\"./images/week6/layer2.png\" alt=\"layer1\" style=\"width:400px\">  \n",
    "\n",
    "- Visualization of 2nd layer in CNN\n",
    "- More complex characteristics like circles and stripes, corners\n",
    "- Left: what the CNN sees based on corresponding images from grid on right\n",
    "- **The CNN learns to do this on its own**\n",
    "    - No special instruction for CNN to focus on more complex layers in deeper layers\n",
    "    \n",
    "## Layer 3\n",
    "<img src=\"./images/week6/layer3.png\" alt=\"layer1\" style=\"width:500px\">  \n",
    "\n",
    "- Third layer picks out complex combinations of features from second layer\n",
    "- Grids, honeycombs (top left), wheels, faces\n",
    "\n",
    "## Layer 5\n",
    "<img src=\"./images/week6/layer5.png\" alt=\"layer1\" style=\"width:400px\">  \n",
    "\n",
    "- Fifth and final layer (skip layer 4)\n",
    "- Last layer picks out highest order ideas\n",
    "- Dog faces, bird faces, bicycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Convolutional Layer\n",
    "\n",
    "- Tensorflow provides `tf.nn.conv2d()` and `tf.nn.bias_add()` to create our own conv layers\n",
    "- TF uses a `stride` for each input dimension\n",
    "    - `[batch, input_height,input_width, input_channels]`\n",
    "    - We generally set 1st and 4th elment to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output depth\n",
    "k_output = 64\n",
    "\n",
    "#Image properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "#Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "#input/image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape = [None,image_width, image_height, color_channels]\n",
    ")\n",
    "\n",
    "#weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_width,filter_size_height, color_channels,\n",
    "    k_output]\n",
    "))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "#Apply convolution\n",
    "conv_layer = tf.nn.conv2d(input,weight,strides=[1,2,2,1],\n",
    "                         padding = 'SAME')\n",
    "#add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "#apply activaion function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve convnets\n",
    "\n",
    "- Pooling\n",
    "- 1x1 convolutions\n",
    "- Inception architecture\n",
    "\n",
    "## Pooling\n",
    "<img src=\"./images/week6/pooling.png\" alt=\"pooling\" style=\"width:400px\">  \n",
    "\n",
    "- Striding shifts pixels a bit at a time to reduce feature map size\n",
    "    - This is very aggressive\n",
    "    - Removes a lot of information\n",
    "- What if we took small strides (say 1 step)\n",
    "- Then took all the convolutions in a neighbour and combined them somehow\n",
    "    - This is known as **Pooling**\n",
    "\n",
    "### Max Pooling\n",
    "<img src=\"./images/week6/maxpooling.png\" alt=\"max-pooling\" style=\"width:400px\">  \n",
    "\n",
    "- Max pooling is most common\n",
    "- At every point on feature map, look at small neighbour around that point, and compute the maximum of those points\n",
    "- This is **parameter-free**!\n",
    "- Often more accurate\n",
    "- However, it is more _expensive_ \n",
    "    - Due to shorter convolution strides\n",
    "- More hyperparamters to worry about\n",
    "    - Pooling Size\n",
    "    - Pooling Stride\n",
    "    - They do not have to be the same\n",
    "\n",
    "**A Common architecture for classifying CNNs is:**\n",
    "<img src=\"./images/week6/cnn-layers.png\" alt=\"cnn-layers\" style=\"width:250px\">  \n",
    "\n",
    "- \"`LENET-5`\" \n",
    "    - First successful use was Yan Lecun in 1998 \n",
    "        - Character recognition        \n",
    "- \"`ALEXNET`\"  \n",
    "    - Alex Krizhevsky 2012\n",
    "    - Won the object recognition challenge in 2012\n",
    "<img src=\"./images/week6/example-networks.png\" alt=\"example-networks\" style=\"width:250px\">  \n",
    "\n",
    "### Average Pooling\n",
    "- Take an average over a window of pixels around a specific location\n",
    "- Similar to providing a blurred low resolution view of the feature map below\n",
    "<img src=\"./images/week6/avg-pooling.png\" alt=\"average-pooling\" style=\"width:250px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Max Pooling\n",
    "<img src=\"./images/week6/ex-max.png\" alt=\"example-max-pooling\" style=\"width:250px\">  \n",
    "\n",
    "- Source Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons\n",
    "- Example max pooling\n",
    "- 2x2 filter and a stride of 2\n",
    "- Four colours represent each time the filter was aplied to find the maximum value\n",
    "- Used to reduce the size of the input, and allow the net to focus on only the most important elments\n",
    "- `tf.nn.max_pool()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers today\n",
    "\n",
    "- Fallen out of favor\n",
    "- Recent datasets are so big and complex that we're more concerned about _underfitting_\n",
    "- **Dropout is a much better regularizer**\n",
    "- Pooling resultsin a loss of information\n",
    "- Think about max pooling operation as an example\n",
    "    - We only keep the largest of `n` numbers\n",
    "    - Disregards `n-1` numbers completely\n",
    "    \n",
    "- `new_height = (input_height - filter_height)/S +1`\n",
    "- `new_width = (input_width - filter_width)/S +1`\n",
    "- for pooling layer, depth is same as input depth\n",
    "- Pooling operation is applied individually for each depth slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 convolutions\n",
    "<img src=\"./images/week6/onebyone.png\" alt=\"1x1-convolution\" style=\"width:350px\">  \n",
    "\n",
    "- Not really looking at patch of image, just one pixel\n",
    "- Classic convolution is _basically_ a small classifier for a patch of the image\n",
    "    - Only a linear classifier\n",
    "- If we add a 1x1 convolution in the middle\n",
    "    - Suddenly we have a mini neural network learning over the patch before the linear classifier\n",
    "- Interspersing convolutions with 1x1 convlutions is a very inexpensive way to make model deeper and have more parameters withot completely changing the structure\n",
    "- **Very cheap**\n",
    "    - 1x1 convolutions <=> matrix multiplies\n",
    "- We went through pooling and 1x1 convolutions because in recent years there is a new technique that results in smaller + better nets\n",
    "    - Simply a pyramid of convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Modules\n",
    "- At each layer of convnet, we can decide\n",
    "    - Do we want a convolution?\n",
    "        - 1x1?\n",
    "        - 3x3?\n",
    "        - 5x5?\n",
    "    - Pooling computation?\n",
    "- It turns out that all of these are beneficial to the power of the network\n",
    "- _**So why choose?**_\n",
    "- **Use them all!**\n",
    "\n",
    "<img src=\"./images/week6/inception.png\" alt=\"inception module\" style=\"width:350px\">  \n",
    "\n",
    "- Average pool followed by 1x1\n",
    "- 1x1 conv\n",
    "- 1x1 followed by 3x3\n",
    "- 1x1 followed by 5x5\n",
    "- Then concatenate all of them at the end\n",
    "- Can choose the parameters such that the total # of parameters in our model is very small\n",
    "- Yet the model performs better than if we had a simple convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network in TensorFlow\n",
    "- found in separate file `cnn.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
