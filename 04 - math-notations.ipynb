{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Math Behind Deep Learning\n",
    "Notes taken from Siraj's video: https://youtu.be/N4gDikiec8E\n",
    "\n",
    "The math that we will mainly need to fully understand Deep Learning are 1) statistics, 2) calculus, and 3) linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math cheat sheets\n",
    "\n",
    "**Statistics:**\n",
    "http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf\n",
    "\n",
    "**Linear Algebra:**\n",
    "http://www.souravsengupta.com/cds2016/lectures/Savov_Notes.pdf   \n",
    "\n",
    "**Calculus:**\n",
    "http://tutorial.math.lamar.edu/pdf/Calculus_Cheat_Sheet_All.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Step Pipeline\n",
    "\n",
    "1. Collect data\n",
    "2. Build Model\n",
    "3. Train Model\n",
    "4. Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "### Weight initialization\n",
    "\n",
    "- We usually randomly initialize our weights and biases by drawing from a Normal distribution\n",
    "\n",
    "\n",
    "### Normalization\n",
    "\n",
    " - Scaling the features of our data\n",
    " \n",
    "Example:\n",
    "\n",
    "- min max scaling (popular)\n",
    "    - scale the each feature such that each data point ranges between [0,1]\n",
    "    $$z = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
    "    \n",
    "- z-scoring\n",
    "    - scale such that the mean of each feature is zero and standard deviation is 1\n",
    "    - for each feature, subtract each datapoint by the mean and divide by the standard deviation\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{\\sigma_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "Essential linear algebra terms often used in Deep learning:\n",
    "\n",
    "1. Scalar\n",
    "    - single number (e.g. $x$)\n",
    "2. Vector\n",
    "    - a 1-dimensional array of numbers\n",
    "    \n",
    "    \\begin{equation}\n",
    "     X=\\begin{bmatrix}\n",
    "         x_{1} \\\\\n",
    "         x_{2} \\\\\n",
    "         \\vdots \\\\\n",
    "         x_{n}\n",
    "        \\end{bmatrix}\n",
    "  \\end{equation}\n",
    "  \n",
    "3. Matrix\n",
    "    - 2-dimensional array of numbers\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\mathbf{X} = \n",
    "    \\begin{bmatrix}\n",
    " x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    " x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    " \\end{bmatrix}\n",
    "   \\end{equation}\n",
    "\n",
    "4. Tensor\n",
    "    - N-dimensional array of numbers\n",
    "    - Tensors are actually the general form of all the above mentioned math objects.\n",
    "        - 0-dimensional $\\rightarrow$ scalar\n",
    "        - 1-dimensional $\\rightarrow$ vector\n",
    "        - 2-dimensional $\\rightarrow$ matrix\n",
    "        \n",
    "Linear algebra-based math is heavily used for both forward and back propagation of neural networks. \n",
    "\n",
    "For example:\n",
    "\n",
    "- Matrix multiplication\n",
    "- Dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "\n",
    "- The vast majority of the calculus used in Deep Learning is (multivariable) differential calculus\n",
    "- This would be covered from calc 1-3 in most post-secondary schools\n",
    "- We use derivatives to compute the _gradient_ of our error and cost function\n",
    "\n",
    "For example:\n",
    "\n",
    "- One of the main activation functions we have used so far is the sigmoid function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- The derivative of this function, which we require in order to backpropagate the model's calculated error, is\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x)  (1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Let's use a 3-layer feedforward neural net to illustrate what we've talked about so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 1: Collect Data\n",
    "\n",
    "#input data\n",
    "x = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "\n",
    "#output data\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter examples\n",
    "\n",
    "- When our models get more complicated, we'll have to worry about lots of hyperparameters and how to tune/optimize them.\n",
    "- Like tiny user-defined tuning knobs of our network\n",
    "- e.g. how many fwd/bckwd iterations, how many hidden layers, how many neurons per layer, etc.\n",
    "\n",
    "<img src=\"./images/week4/hyperparam.png\" alt=\"Hyper parameters\" style=\"width:400px\">\n",
    "\n",
    "### Random Search\n",
    "\n",
    "- define ranges of values for each hyperparameter\n",
    "- e.g.\n",
    "    - 0.001 < Learning Rate < 0.003\n",
    "    - 3 < # of layers < 120\n",
    "- then create a search algorithm that randomly picks values between these values\n",
    "    - uniformly distributed between the lower and upper bound that we defined\n",
    "        - all possible values have the same probability of being chosen\n",
    "        \n",
    "<img src = \"./images/week4/uniform.png\" alt=\"uniform distribution\" style=\"width:250px\">        \n",
    "\n",
    "One common way is to draw from uniform distribution with low deviation (values are close together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 2: build model\n",
    "num_epochs = 60000\n",
    "\n",
    "#initialize weights\n",
    "\n",
    "# -1 < weight values < 1 \n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22174918  0.2370225  -0.71222459 -0.11069419]\n",
      " [-0.8950748  -0.89992915 -0.40739545  0.93409891]\n",
      " [ 0.73636171 -0.94204698 -0.95417025 -0.38501225]]\n"
     ]
    }
   ],
   "source": [
    "#Every node in input layer will be connected to \n",
    "#every node in the next layer\n",
    "print(syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39602391]\n",
      " [-0.84302219]\n",
      " [-0.13352481]\n",
      " [-0.84793483]]\n"
     ]
    }
   ],
   "source": [
    "#dimension 4x1, size of our output\n",
    "print(syn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "- Initialize random weights for our neurons\n",
    "\n",
    "#### Forward propagation\n",
    "- Compute forward pass \n",
    "- Pass inputs to hidden layer weighted by our randomly initialized weights\n",
    "- Use sigmoid function (defined above) as our activation function in order to squash values into probabilities bounded between 0 and 1\n",
    "<img src=\"./images/week4/fwd.png\" alt=\"forward-pass\" style=\"width: 200px\">\n",
    "\n",
    "#### Gradient descent and backpropagation\n",
    "- Calculate the difference between our model's output and the true $y$ values\n",
    "    - i.e. calculate the error\n",
    "    - think of the error as a n-dimensional bowl that we want to get to the bottom of (minimize)\n",
    "    - we can compute the derivative of where we are in the bowl to help guide us towards the minimum\n",
    "    - repeat this process many times until our model converges (error stops decreasing)\n",
    "<img src=\"./images/week4/minimize-error.png\" alt=\"minimize\" style=\"width: 400px\">        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define sigmoid activation function\n",
    "#add boolean argument to use its derivative as well\n",
    "# (useful for backwards pass)\n",
    "def nonlin(x, deriv = False):\n",
    "    if(deriv):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.50076597667\n",
      "Error:0.0101335121582\n",
      "Error:0.00671074939929\n",
      "Error:0.00531179978552\n",
      "Error:0.004511936752\n",
      "Error:0.00398076795327\n"
     ]
    }
   ],
   "source": [
    "#Step 3: train model\n",
    "\n",
    "for jj in range(num_epochs):\n",
    "    #feed forward through layers 0,1,2\n",
    "    l0 = x\n",
    "    #squash into output probabilities using our nonlinearity\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "    \n",
    "    #how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (jj% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    #we want to minimize this error\n",
    "    #the smaller the error, the better our prediction\n",
    "    #we can't change our input data but we CAN change our\n",
    "    #weights to help minimize this error\n",
    "    \n",
    "    l2_delta = l2_error * nonlin(l2,deriv=True)\n",
    "    \n",
    "    #BACKPROP\n",
    "    #how much did each l1 value contribute to the l2 error\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    \n",
    "    #update weights \n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each iteration, we can see that the error decreased. This means our model was learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Deep learning uses Linear Algebra, Statistics, and Calculus\n",
    "2. A Neural net performs a series of operations on an input *tensor* to make a prediction\n",
    "3. We can optimize a prediction using gradient descent to backpropagate errors recursively and update weights accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental resources:\n",
    "Math for ML:\n",
    "\n",
    "- https://people.ucsc.edu/~praman1/static/pub/math-for-ml.pdf\n",
    "- http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf\n",
    "- http://datascience.ibm.com/blog/the-mathematics-of-machine-learning/\n",
    "\n",
    "Part 1 of Goodfellow's graduate-level textbook\n",
    "\n",
    "- http://www.deeplearningbook.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
